# Generative AI

Explorations into the frontier of Artificial Intelligence.

## Core Concepts

*   [**Transformer & Attention Foundations**](transformer-attention.md): The architecture that started it all. A mathematical and code-level deep dive into Self-Attention, Multi-Head Attention, and Positional Encodings.

## Coming Soon

*   **Diffusion Models**: From DDPM to Stable Diffusion.
*   **LLM Alignment**: RLHF and DPO.

